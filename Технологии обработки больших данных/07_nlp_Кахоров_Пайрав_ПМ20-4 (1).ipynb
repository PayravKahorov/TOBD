{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Введение в обработку текста на естественном языке\"\n",
    "* https://www.nltk.org/api/nltk.metrics.distance.html\n",
    "* https://pymorphy2.readthedocs.io/en/stable/user/guide.html\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Загрузите данные из файла `ru_recipes_sample.csv` в виде `pd.DataFrame` `recipes` Используя регулярные выражения, удалите из описаний (столбец `description`) все символы, кроме русских букв, цифр и пробелов. Приведите все слова в описании к нижнему регистру. Сохраните полученный результат в столбец `description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('ru_recipes_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ru(a):\n",
    "    p = re.compile(f'[а-яА-Я\\s\\d]')\n",
    "    return \"\".join(p.findall(a)).lower()\n",
    "    #p = re.findall(f'[а-яА-Я]')\n",
    "    #return \"\".join(re.findall(f'[а-яА-Я\\s\\d]',a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].apply(find_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       этот коктейль готовлю из замороженной клубники...\n",
       "1                                         быстро и вкусно\n",
       "2                сытный овощной салатик пальчики оближете\n",
       "3       картофельное пюре и куриные котлеты  вкусная к...\n",
       "4       вишневая наливка имеет яркий вишневый вкус кот...\n",
       "                              ...                        \n",
       "3462    для тех кто любит чечевицу вам сюда очень вкус...\n",
       "3463    баклажановые фантазии продолжаются предлагаю в...\n",
       "3464    мое любимое блюдо лазанья но кушать только фар...\n",
       "3465    прошлым летом варила варенье из одуванчиков по...\n",
       "3466     и три корочки хлеба  сделал заказ буратино в ...\n",
       "Name: description, Length: 3467, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние Левенштейна. Выведите на экран результат в следующем виде:\n",
    "\n",
    "```\n",
    "d(word1, word2) = x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_token(a):\n",
    "#     return word_tokenize(a)\n",
    "# word_token('I love you')\n",
    "unique_words = list(set(sum(list(map(word_tokenize,df['description'])),[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(преследуется,кабачковом) = 12\n",
      "d(детском,любых) = 7\n",
      "d(смаченками,шоколаде) = 9\n",
      "d(меда,малина) = 4\n",
      "d(добрыми,зубчиков) = 7\n"
     ]
    }
   ],
   "source": [
    "rand_words = np.random.choice(unique_words,10)\n",
    "for i in range(5):\n",
    "    rand_1 = np.random.choice(unique_words,1)[0]\n",
    "    rand_2 = np.random.choice(unique_words,1)[0]\n",
    "    edit_dst = edit_distance(rand_1,rand_2)\n",
    "    print(f'd({rand_1},{rand_2}) = {edit_dst}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите функцию, которая принимает на вход 2 текстовые строки `s1` и `s2` и при помощи расстояния Левенштейна определяет, является ли строка `s2` плагиатом `s1`. Функция должна реализовывать следующую логику: для каждого слова `w1` из `s1` проверяет, есть в `s2` хотя бы одно слово `w2`, такое, что расстояние Левенштейна между `w1` и `w2` меньше 2, и считает количество таких слов в `s1` $P$. \n",
    "\n",
    "$$ P = \\#\\{w_1 \\in s_1\\ | \\exists w_2 \\in s_2 : d(w_1, w_2) < tol\\}$$\n",
    "\n",
    "$$ L = max(|s1|, |s2|) $$\n",
    "\n",
    "Здесь $|\\cdot|$ - количество слов в строке, $\\#A$ - число элементов в множестве $A$, $w \\in s$ означает, что слово $w$ содержится в тексте $s$.\n",
    "\n",
    "Если отношение $P / L$ больше 0.8, то функция должна вернуть True; иначе False.\n",
    "\n",
    "Продемонстрируйте работу вашей функции на примере описаний двух рецептов с ID 135488 и 851934 (ID рецепта - это число, стоящее в конце url рецепта). Выведите на экран описания этих рецептов и результат работы функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_re(a):\n",
    "    p = re.compile(r'851934|135488')\n",
    "    if p.findall(str(a)) != []:\n",
    "        return  1 #p.findall(str(a))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url    3465\n",
       "dtype: int64"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df['url'].apply(func_re)).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plagiarism(s1: str, s2: str) -> bool:\n",
    "    P = 0\n",
    "    L = max(len(word_tokenize(s1)),len(word_tokenize(s2)))\n",
    "    for w1 in word_tokenize(s1):\n",
    "        for w2 in word_tokenize(s2):\n",
    "            if edit_distance(w1,w2) < 2:\n",
    "                P += 1 #s1.count(j)\n",
    "                #print(w1,w2)\n",
    "                break\n",
    "                #print(s1.count(j),j)\n",
    "    if P/L > 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plagiarism(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in word_tokenize(s1)]\n",
    "# [i for i in word_tokenize(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1 = df.loc[1473][3]#df['description'][2]\n",
    "# s2 = df.loc[958][3]#df['description'][5]\n",
    "# P = 0\n",
    "# L = 0\n",
    "# for i in word_tokenize(s1):\n",
    "#     for j in word_tokenize(s2):\n",
    "#         if edit_distance(i,j) < 2:\n",
    "#             #print(s1.count(i),i)\n",
    "#             P = s1.count(i)\n",
    "#             L = max(len(word_tokenize(s1)),len(word_tokenize(s2)))\n",
    "# P/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. На основе набора слов из задания 2 создайте `pd.DataFrame` со столбцами `word`, `stemmed_word` и `normalized_word`. В столбец `stemmed_word` поместите версию слова после проведения процедуры стемминга; в столбец `normalized_word` поместите версию слова после проведения процедуры лемматизации. Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга можно воспользоваться `SnowballStemmer` из `nltk`, для лемматизации слов - пакетом `pymorphy2`. Сравните результаты стемминга и лемматизации. Поясните на примере одной из строк получившегося фрейма (в виде текстового комментария), в чем разница между двумя этими подходами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word = [morph.parse(w)[0].normalized.word for w in unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_word = [stemmer.stem(w) for w in unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(stemmed_word, columns=['stemmed_word'],index = unique_words)\n",
    "df_1['normalized_word'] = normalized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>кексы</th>\n",
       "      <td>кекс</td>\n",
       "      <td>кекс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>сметаны</th>\n",
       "      <td>смета</td>\n",
       "      <td>сметана</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>соседей</th>\n",
       "      <td>сосед</td>\n",
       "      <td>сосед</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ансамбль</th>\n",
       "      <td>ансамбл</td>\n",
       "      <td>ансамбль</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>нужная</th>\n",
       "      <td>нужн</td>\n",
       "      <td>нужный</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stemmed_word normalized_word\n",
       "кексы            кекс            кекс\n",
       "сметаны         смета         сметана\n",
       "соседей         сосед           сосед\n",
       "ансамбль      ансамбл        ансамбль\n",
       "нужная           нужн          нужный"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Добавьте в таблицу `recipes` столбец `description_no_stopwords`, в котором содержится текст описания рецепта после удаления из него стоп-слов. Посчитайте и выведите на экран долю стоп-слов среди общего количества слов. Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Payrav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = stopwords.words('russian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_stop_words(a):\n",
    "    return [word for word in word_tokenize(str(a)) if word not in new_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['description_no_stopwords'] = df['description'].apply(desc_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_1 = len(list(sum(list(map(word_tokenize,df['description'])),[]))) #Кол-во слов без удаления стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sum(list(map(len, recipes['description_no_stopwords']))) #Кол-во слов после удаления стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.267086173745845 %\n"
     ]
    }
   ],
   "source": [
    "print(f'{(unique_words_1 - m) * 100/ unique_words} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array(sum(list(map(word_tokenize, df['description'])),[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'с', 'на', 'очень', 'не', 'из', 'я', 'рецепт', 'а']"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq, counts = np.unique(l, return_counts= True)\n",
    "top_words= dict(sorted(dict(zip(uniq,counts)).items(), key=lambda x: x[1], reverse=True)) #top 10 до\n",
    "list(top_words.keys())[:10] #top 10 до"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_2 = np.array(sum(list(map(word_tokenize, recipes['description_no_stopwords'])),[]))\n",
    "# uniq, counts = np.unique(l_2, return_counts= True)\n",
    "# # sorted(counts,reverse=True)\n",
    "# top_words= dict(sorted(dict(zip(uniq,counts)).items(), key=lambda x: x[1], reverse=True)) #top 10 до\n",
    "# list(top_words.keys())[:10] #top 10 до"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipes['description_no_stopwords'].apply(length).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def length(b):\n",
    "#     return len(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Выберите случайным образом 5 рецептов из набора данных, в названии которых есть слово \"оладьи\" (без учета регистра). Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`. На основе полученных векторов создайте `pd.DataFrame`, в котором названия колонок соответствуют словам из словаря объекта-векторизатора. \n",
    "\n",
    "Примечание: обратите внимание на порядок слов при создании колонок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ru(a):\n",
    "    p = re.compile(f'Куриные')\n",
    "    return p.findall(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Куриные']"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_ru(df['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      Густой молочно-клубничный коктейль\n",
       "1                                                Рулетики\n",
       "2                                     Салат \"Баклажанчик\"\n",
       "3           Куриные котлеты с картофельным пюре в духовке\n",
       "4                                 Рецепт вишневой наливки\n",
       "                              ...                        \n",
       "3462                                                 Мшош\n",
       "3463                     Мясные треугольники с баклажаном\n",
       "3464                                 \"Болоньез\" по-новому\n",
       "3465                 Варенье из одуванчиков с апельсинами\n",
       "3466    Три корочки хлеба под соусом болоньез для Пино...\n",
       "Name: name, Length: 3467, dtype: object"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Вычислите близость между каждой парой рецептов, выбранных в задании 6, используя косинусное расстояние (можно воспользоваться функциями из любого пакета: `scipy`, `scikit-learn` или реализовать функцию самому). Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов.\n",
    "\n",
    "Примечание: обратите внимание, что $d_{cosine}(x, x) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию, которая принимает на вход `pd.DataFrame`, полученный в задании 7, и возвращает в виде кортежа названия двух различных рецептов, которые являются наиболее похожими. Прокомментируйте результат (в виде текстового комментария). Для объяснения результата сравните слова в описаниях двух этих отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(sim_df: pd.DataFrame) -> tuple:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
